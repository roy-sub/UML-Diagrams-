{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:27:31.961050Z","iopub.status.busy":"2024-03-02T17:27:31.960810Z","iopub.status.idle":"2024-03-02T17:27:42.683182Z","shell.execute_reply":"2024-03-02T17:27:42.682155Z","shell.execute_reply.started":"2024-03-02T17:27:31.961028Z"},"trusted":true},"outputs":[],"source":["import os\n","import collections\n","import pandas as pd\n","import numpy as np\n","import functools\n","import matplotlib.pyplot as plt\n","import cv2\n","\n","from sklearn import preprocessing \n","\n","import xml.etree.ElementTree as ET\n","\n","import albumentations as A\n","from albumentations.pytorch.transforms import ToTensorV2\n","\n","import torch\n","import torchvision\n","\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, fasterrcnn_resnet50_fpn_v2, fasterrcnn_resnet50_fpn\n","from torchvision.models.detection import FasterRCNN_ResNet50_FPN_Weights, FasterRCNN_ResNet50_FPN_V2_Weights\n","from torchvision.models import resnet50, ResNet50_Weights\n","from torchvision.models.detection.rpn import AnchorGenerator\n","\n","from torch.utils.data import DataLoader, Dataset\n","from torch.utils.data import SequentialSampler\n","\n","from PIL import Image\n","import seaborn as sns\n","import copy\n","\n","import torchmetrics\n","from torchmetrics.detection import MeanAveragePrecision\n","from engine import train_one_epoch, evaluate\n","import utils\n","import torchvision.transforms.functional as tf\n","import wandb\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:27:42.686099Z","iopub.status.busy":"2024-03-02T17:27:42.685357Z","iopub.status.idle":"2024-03-02T17:27:42.691948Z","shell.execute_reply":"2024-03-02T17:27:42.690369Z","shell.execute_reply.started":"2024-03-02T17:27:42.686065Z"},"trusted":true},"outputs":[],"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","from collections import defaultdict, deque\n","import datetime\n","import time\n","from tqdm import tqdm \n","from torchvision.utils import draw_bounding_boxes\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:07.214144Z","iopub.status.busy":"2024-03-02T17:28:07.213358Z","iopub.status.idle":"2024-03-02T17:28:07.253540Z","shell.execute_reply":"2024-03-02T17:28:07.252862Z","shell.execute_reply.started":"2024-03-02T17:28:07.214104Z"},"trusted":true},"outputs":[],"source":["import shutil\n","import os\n","source_dir = 'Construction/train'\n","img_dir = \"custom_dataset/Images\"\n","annot_dir = \"custom_dataset/Annotations\"\n","\n","os.makedirs(img_dir, exist_ok=True)\n","os.makedirs(annot_dir, exist_ok=True)\n","\n","for filename in os.listdir(source_dir):\n","    if filename.endswith(\".jpg\"):\n","        shutil.move(os.path.join(source_dir, filename), os.path.join(img_dir, filename))\n","    \n","    elif filename.endswith(\".xml\"):\n","        shutil.move(os.path.join(source_dir, filename), os.path.join(annot_dir, filename))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:07.254749Z","iopub.status.busy":"2024-03-02T17:28:07.254491Z","iopub.status.idle":"2024-03-02T17:28:07.262349Z","shell.execute_reply":"2024-03-02T17:28:07.261489Z","shell.execute_reply.started":"2024-03-02T17:28:07.254726Z"},"trusted":true},"outputs":[],"source":["train_transform=A.Compose([A.HorizontalFlip(),\n","                           A.ShiftScaleRotate(rotate_limit=15,value=0,\n","                                              border_mode=cv2.BORDER_CONSTANT),\n","\n","                           A.OneOf(\n","                                   [A.CLAHE(),\n","                                    A.RandomBrightnessContrast(),\n","                                    A.HueSaturationValue()],p=1),\n","                           A.GaussNoise(),\n","                           A.RandomResizedCrop(height=480,width=480)],\n","                          bbox_params=A.BboxParams(format=\"pascal_voc\",min_visibility=0.15,\n","                                                   label_fields=[\"labels\"]))\n","                           \n","val_transform=A.Compose([A.Resize(height=480,width=480)],\n","                        bbox_params=A.BboxParams(format=\"pascal_voc\",min_visibility=0.15,\n","                                                 label_fields=[\"labels\"]))\n","test_transform=A.Compose([A.Resize(height=480,width=480)],\n","                        bbox_params=A.BboxParams(format=\"pascal_voc\",min_visibility=0.15,\n","                                                 label_fields=[\"labels\"]))                                                 "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:07.264221Z","iopub.status.busy":"2024-03-02T17:28:07.263639Z","iopub.status.idle":"2024-03-02T17:28:07.329626Z","shell.execute_reply":"2024-03-02T17:28:07.328692Z","shell.execute_reply.started":"2024-03-02T17:28:07.264186Z"},"trusted":true},"outputs":[],"source":["\n","classes=[\"background\",\n","    'Excavator',\n","    'Gloves',\n","    'Hardhat',\n","    'Ladder',\n","    'Mask',\n","    'NO-Hardhat',\n","    'NO-Mask',\n","    'NO-Safety Vest',\n","    'Person',\n","    'SUV',\n","    'Safety Cone',\n","    'Safety Vest',\n","    'bus',\n","    'dump truck',\n","    'fire hydrant',\n","    'machinery',\n","    'mini-van',\n","    'sedan',\n","    'semi',\n","    'trailer',\n","    'truck',\n","    'truck and trailer',\n","    'van',\n","    'vehicle',\n","    'wheel loader'\n","]\n","\n","num_classes=len(classes)\n","device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:07.331157Z","iopub.status.busy":"2024-03-02T17:28:07.330885Z","iopub.status.idle":"2024-03-02T17:28:07.339871Z","shell.execute_reply":"2024-03-02T17:28:07.338996Z","shell.execute_reply.started":"2024-03-02T17:28:07.331134Z"},"trusted":true},"outputs":[],"source":["def parse_xml(annot_path):\n","    tree=ET.parse(annot_path)\n","    root=tree.getroot()\n","    \n","    width=int(root.find(\"size\").find(\"width\").text)\n","    height=int(root.find(\"size\").find(\"height\").text)\n","    boxes=[]\n","    \n","    for obj in root.findall(\"object\"):\n","        bbox=obj.find(\"bndbox\")\n","        xmin=int(bbox.find(\"xmin\").text)\n","        ymin=int(bbox.find(\"ymin\").text)\n","        xmax=int(bbox.find(\"xmax\").text)\n","        ymax=int(bbox.find(\"ymax\").text)\n","        \n","        boxes.append([xmin,ymin,xmax,ymax])\n","        \n","    return boxes,height,width"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:07.341610Z","iopub.status.busy":"2024-03-02T17:28:07.341049Z","iopub.status.idle":"2024-03-02T17:28:07.450724Z","shell.execute_reply":"2024-03-02T17:28:07.450074Z","shell.execute_reply.started":"2024-03-02T17:28:07.341579Z"},"trusted":true},"outputs":[],"source":["ignore_img=[]\n","for annot_name in os.listdir(annot_dir):\n","    img_name=annot_name[:-4]+\".jpg\"\n","    annot_path=os.path.join(annot_dir,annot_name)\n","    boxes,height,width=parse_xml(annot_path)\n","    \n","    for box in boxes:\n","        if box[0]<0 or box[0]>=box[2] or box[2]>width:\n","            ignore_img.append(img_name)\n","        elif box[1]<0 or box[1]>=box[3] or box[3]>height:\n","            ignore_img.append(img_name)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:07.452200Z","iopub.status.busy":"2024-03-02T17:28:07.451874Z","iopub.status.idle":"2024-03-02T17:28:07.463984Z","shell.execute_reply":"2024-03-02T17:28:07.463073Z","shell.execute_reply.started":"2024-03-02T17:28:07.452171Z"},"trusted":true},"outputs":[],"source":["class VOCDataset(Dataset):\n","    def __init__(self,img_dir,annot_dir,transform=None):\n","        super().__init__()\n","        self.img_dir=img_dir\n","        self.annot_dir=annot_dir\n","        self.img_list=sorted([img for img in os.listdir(self.img_dir) \n","                              if img not in ignore_img])\n","        self.transform=transform\n","        \n","    def __len__(self):\n","        return len(self.img_list)\n","    \n","    def __getitem__(self,idx):\n","        img_name=self.img_list[idx]\n","        img_path=os.path.join(self.img_dir,img_name)\n","        img=cv2.imread(img_path)\n","        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n","        \n","        annot_name=img_name[:-4]+\".xml\"\n","        annot_path=os.path.join(self.annot_dir,annot_name)\n","        boxes,height,width=parse_xml(annot_path)\n","        labels=[1]*len(boxes)\n","        \n","        if self.transform is not None:\n","            transformed=self.transform(image=img,bboxes=boxes,labels=labels)\n","            img=transformed[\"image\"]\n","            boxes=transformed[\"bboxes\"]\n","            labels=transformed[\"labels\"]\n","        \n","        if len(np.array(boxes).shape)!=2 or np.array(boxes).shape[-1]!=4:\n","            boxes=[[0.0,0.0,1.0,1.0]]\n","            labels=[0]\n","                \n","        img=img/255\n","        img=tf.to_tensor(img)\n","        img=img.to(dtype=torch.float32)\n","        target={}\n","        target[\"boxes\"]=torch.tensor(boxes,dtype=torch.float32)\n","        target[\"labels\"]=torch.tensor(labels,dtype=torch.int64)\n","        target[\"id\"]=torch.tensor(idx)\n","            \n","        return img,target\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:07.468608Z","iopub.status.busy":"2024-03-02T17:28:07.468341Z","iopub.status.idle":"2024-03-02T17:28:07.477443Z","shell.execute_reply":"2024-03-02T17:28:07.476526Z","shell.execute_reply.started":"2024-03-02T17:28:07.468586Z"},"trusted":true},"outputs":[],"source":["def collate_fn(batch):\n","    return tuple(zip(*batch))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:07.478921Z","iopub.status.busy":"2024-03-02T17:28:07.478532Z","iopub.status.idle":"2024-03-02T17:28:07.493185Z","shell.execute_reply":"2024-03-02T17:28:07.492468Z","shell.execute_reply.started":"2024-03-02T17:28:07.478892Z"},"trusted":true},"outputs":[],"source":["train_ds=VOCDataset(img_dir,annot_dir,train_transform)\n","val_ds=VOCDataset(img_dir,annot_dir,val_transform)\n","test_ds=VOCDataset(img_dir,annot_dir,test_transform)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:07.494499Z","iopub.status.busy":"2024-03-02T17:28:07.494196Z","iopub.status.idle":"2024-03-02T17:28:07.498670Z","shell.execute_reply":"2024-03-02T17:28:07.497831Z","shell.execute_reply.started":"2024-03-02T17:28:07.494477Z"},"trusted":true},"outputs":[],"source":["from torch.utils.data import Subset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:07.500054Z","iopub.status.busy":"2024-03-02T17:28:07.499718Z","iopub.status.idle":"2024-03-02T17:28:07.509548Z","shell.execute_reply":"2024-03-02T17:28:07.508722Z","shell.execute_reply.started":"2024-03-02T17:28:07.500027Z"},"trusted":true},"outputs":[],"source":["idxs=list(range(len(train_ds)))\n","\n","np.random.shuffle(idxs)\n","train_idx=idxs[:int(0.7*len(train_ds))]\n","val_idx=idxs[int(0.2*len(val_ds)):]\n","test_idx=idxs[int(0.1*len(test_ds)):]\n","\n","train_ds=Subset(train_ds,train_idx)\n","val_ds=Subset(val_ds,val_idx)\n","test_ds=Subset(test_ds,test_idx)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:07.510846Z","iopub.status.busy":"2024-03-02T17:28:07.510569Z","iopub.status.idle":"2024-03-02T17:28:07.519696Z","shell.execute_reply":"2024-03-02T17:28:07.518881Z","shell.execute_reply.started":"2024-03-02T17:28:07.510824Z"},"trusted":true},"outputs":[],"source":["batch_size=2\n","train_dl=DataLoader(train_ds,batch_size=batch_size,shuffle=True,num_workers=os.cpu_count(),\n","                    collate_fn=collate_fn,\n","                    pin_memory=True if device==\"cuda\" else False)\n","val_dl=DataLoader(val_ds,batch_size=batch_size,shuffle=False,num_workers=os.cpu_count(),\n","                  collate_fn=collate_fn,\n","                  pin_memory=True if device==\"cuda\" else False)\n","test_dl=DataLoader(test_ds,batch_size=batch_size,shuffle=False,num_workers=os.cpu_count(),\n","                  collate_fn=collate_fn,\n","                  pin_memory=True if device==\"cuda\" else False,drop_last=True)           "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:09.496439Z","iopub.status.busy":"2024-03-02T17:28:09.496130Z","iopub.status.idle":"2024-03-02T17:28:09.502485Z","shell.execute_reply":"2024-03-02T17:28:09.501450Z","shell.execute_reply.started":"2024-03-02T17:28:09.496414Z"},"trusted":true},"outputs":[],"source":["# Initialize the model with pre-trained weights\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(\n","    pretrained=True,\n","    weights=torchvision.models.detection.FasterRCNN_ResNet50_FPN_Weights.DEFAULT,\n","    weights_backbone=torchvision.models.ResNet50_Weights.IMAGENET1K_V2,\n","    trainable_backbone_layers=5\n",")\n","\n","# Replace the classifier with a new one, for num_classes (25 classes + 1 background)\n","num_classes = 26  # your dataset classes + background\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","model.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:09.503792Z","iopub.status.busy":"2024-03-02T17:28:09.503521Z","iopub.status.idle":"2024-03-02T17:28:09.744203Z","shell.execute_reply":"2024-03-02T17:28:09.743213Z","shell.execute_reply.started":"2024-03-02T17:28:09.503753Z"},"trusted":true},"outputs":[],"source":["params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.001, weight_decay=0.0005)\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:28:32.444278Z","iopub.status.busy":"2024-03-02T17:28:32.443973Z","iopub.status.idle":"2024-03-02T17:29:42.836357Z","shell.execute_reply":"2024-03-02T17:29:42.835028Z","shell.execute_reply.started":"2024-03-02T17:28:32.444249Z"},"trusted":true},"outputs":[],"source":["! git clone https://github.com/pytorch/vision.git\n","! cd vision;cp references/detection/utils.py ../;cp references/detection/transforms.py ../;cp references/detection/coco_eval.py ../;cp references/detection/engine.py ../;cp references/detection/coco_utils.py ../"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:30:25.864344Z","iopub.status.busy":"2024-03-02T17:30:25.863560Z","iopub.status.idle":"2024-03-02T17:30:25.868688Z","shell.execute_reply":"2024-03-02T17:30:25.867754Z","shell.execute_reply.started":"2024-03-02T17:30:25.864311Z"},"trusted":true},"outputs":[],"source":["def get_lr(optimizer):\n","    for params in optimizer.param_groups:\n","        return params[\"lr\"]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["id_to_label = {index: label for index, label in enumerate(classes)}\n","\n","def preprocess_bbox(prediction, threshold):\n","    mask = prediction[\"scores\"] >= threshold\n","    boxes = prediction[\"boxes\"][mask]\n","    scores = prediction[\"scores\"][mask]\n","    labels = prediction[\"labels\"][mask]\n","    nms_indices = torchvision.ops.nms(boxes, scores, iou_threshold=0.5)\n","    return {\"boxes\": boxes[nms_indices], \"scores\": scores[nms_indices], \"labels\": labels[nms_indices]}, scores[nms_indices]\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["wandb.login()\n","\n","wandb.init(\n","    project=\"Faster R-CNN\"\n","    )"]},{"cell_type":"markdown","metadata":{},"source":["## trainer\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","epochs = 5\n","f1_scores_per_threshold = defaultdict(lambda: defaultdict(list))\n","recall_scores_per_threshold = defaultdict(lambda: defaultdict(list))\n","\n","loss_history = {\n","    \"training_loss\": [],\n","    \"validation_loss\": [],\n","    \"box_loss\": [],       # For bounding box loss\n","    \"cls_loss\": [],       # For classification loss\n","    \"dfl_loss\": []        # For direction focal loss or any other specific loss\n","}\n","              \n","all_epoch_results = []\n","\n","train_len=len(train_dl.dataset)\n","val_len=len(val_dl.dataset)\n","test_len=len(test_dl.dataset)\n","\n","best_validation_loss=np.inf\n","best_weights=copy.deepcopy(model.state_dict())\n","\n","for epoch in range(epochs):\n","    \n","    # Initialize losses for this epoch\n","    training_loss = 0.0\n","    validation_loss = 0.0\n","    box_loss = 0.0\n","    cls_loss = 0.0\n","    dfl_loss = 0.0\n","\n","    \n","    current_lr=get_lr(optimizer)\n","    model.train()\n","\n","    for imgs,targets in train_dl:\n","        imgs=[img.to(device) for img in imgs]\n","        targets=[{k:v.to(device) for (k,v) in d.items()} for d in targets]\n","        \n","        loss_dict=model(imgs,targets)\n","        losses=sum(loss for loss in loss_dict.values())\n","        \n","        box_loss += loss_dict['loss_box_reg'].item()\n","        cls_loss += loss_dict['loss_classifier'].item()\n","        #dfl_loss += loss_dict.get('dfl_loss', 0).item()  \n","            \n","        training_loss+=losses.item()\n","        \n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","       \n","    with torch.no_grad():\n","        for imgs,targets in val_dl:\n","            imgs=[img.to(device) for img in imgs]\n","            targets=[{k:v.to(device) for (k,v) in d.items()} for d in targets]\n","            \n","            loss_dict=model(imgs,targets)\n","            losses=sum(loss for loss in loss_dict.values())\n","            validation_loss+=losses.item()\n","            \n","    lr_scheduler.step(validation_loss)\n","    if current_lr!=get_lr(optimizer):\n","        print(\"Loading best Model weights\")\n","        model.load_state_dict(best_weights)\n","    \n","    if validation_loss<best_validation_loss:\n","        best_validation_loss=validation_loss\n","        best_weights=copy.deepcopy(model.state_dict())\n","        print(\"Updating Best Model weights\")\n","        \n","    \n","    loss_history[\"box_loss\"].append(box_loss / len(train_dl))\n","    loss_history[\"cls_loss\"].append(cls_loss / len(train_dl))\n","    #loss_history[\"dfl_loss\"].append(dfl_loss / len(train_dl))\n","  \n","    \n","    print(f\"\\n{epoch+1}/{epochs}\")\n","    print(f\"Training Loss: {training_loss/train_len}\")\n","    print(f\"Validation_loss: {validation_loss/val_len}\")\n","    print(\"\\n\"+\"*\"*50)\n","\n","torch.save(best_weights,\"model_rcnn.pth\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_epoch_results"]},{"cell_type":"markdown","metadata":{},"source":["### Explore the output\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from torchvision import transforms as torchtrans  \n","import matplotlib.patches as patches\n","\n","def torch_to_pil(img):\n","    return torchtrans.ToPILImage()(img).convert('RGB')\n","\n","def plot_img_bbox(img, target):\n","    # plot the image and bboxes\n","    # Bounding boxes are defined as follows: x-min y-min width height\n","    fig, a = plt.subplots(1,1)\n","    fig.set_size_inches(5,5)\n","    a.imshow(img)\n","    for box in (target['boxes']):\n","        x, y, width, height  = box[0], box[1], box[2]-box[0], box[3]-box[1]\n","        rect = patches.Rectangle((x, y),\n","                                 width, height,\n","                                 linewidth = 2,\n","                                 edgecolor = 'r',\n","                                 facecolor = 'none')\n","\n","        # Draw the bounding box on top of the image\n","        a.add_patch(rect)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for i in range(1,3):\n","    img, target = test_ds[i]\n","    # put the model in evaluation mode\n","    model.eval()\n","    with torch.no_grad():\n","        prediction = model([img.to(device)])[0]\n","\n","    plot_img_bbox(torch_to_pil(img), target)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["f1_scores_per_class = defaultdict(lambda: defaultdict(list))\n","recall_scores_per_class = defaultdict(lambda: defaultdict(list))\n","recall_scores_per_class = defaultdict(lambda: defaultdict(list))\n","\n","for epoch_results in all_epoch_results:\n","    for class_idx, class_id in enumerate(epoch_results['classes']):\n","        precision = epoch_results['map_per_class'][class_idx].item()\n","        recall = epoch_results['mar_100_per_class'][class_idx].item()\n","        map_50  = epoch_results['map_50']\n","        # Calculate F1 Score\n","        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","        \n","        # Retrieve the class name using a hypothetical id_to_label function or dictionary\n","        class_name = id_to_label[class_id.item()]\n","\n","        # Log F1 Score for each class and epoch\n","        f1_scores_per_class[class_name][epoch].append(f1_score)\n","        recall_scores_per_class[class_name][epoch].append(recall)\n","        map_scores_per_class[class_name][epoch].append(precision)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["recall_scores_per_threshold"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(1, 3, figsize=(20, 5))  # Adjust to create 3 subplots\n","\n","# Plot F1-Confidence\n","for class_name, f1_scores in f1_scores_per_class.items():\n","    confidences = np.linspace(0, 1, len(f1_scores))  # Replace with actual confidences if available\n","    ax[0].plot(confidences, f1_scores, label=class_name)\n","\n","ax[0].set_xlabel('Confidence')\n","ax[0].set_ylabel('F1')\n","ax[0].set_title('F1-Confidence')\n","ax[0].legend(loc='upper right', bbox_to_anchor=(1.4, 1.0))  # Adjust legend position\n","ax[0].grid(True)\n","\n","# Plot Recall-Confidence\n","for class_name, recall_scores in recall_scores_per_class.items():\n","    confidences = np.linspace(0, 1, len(recall_scores))  # Replace with actual confidences if available\n","    ax[1].plot(confidences, recall_scores, label=class_name)\n","\n","ax[1].set_xlabel('Confidence')\n","ax[1].set_ylabel('Recall')\n","ax[1].set_title('Recall-Confidence')\n","ax[1].legend(loc='upper right', bbox_to_anchor=(1.4, 1.0))  # Adjust legend position\n","ax[1].grid(True)\n","\n","#for class_name, threshold_map_scores in map_scores_per_threshold.items():\n","#    thresholds = list(threshold_map_scores.keys())\n","#    map_scores = [sum(scores) / len(scores) for scores in threshold_map_scores.values()]  # Calculate mean mAP score per threshold\n","#    ax[2].plot(thresholds, map_scores, label=class_name)\n","\n","epochs = range(1, len(all_epoch_results) + 1)\n","map_scores = [epoch_result['map'].item() for epoch_result in all_epoch_results]\n","\n","ax[2].plot(epochs, map_scores, label='mAP')\n","\n","ax[2].set_xlabel('Epoch')\n","ax[2].set_ylabel('mAP')\n","ax[2].set_title('mAP per Epoch')\n","ax[2].legend(loc='upper right', bbox_to_anchor=(1.5, 1.0))  # Adjust legend position\n","ax[2].grid(True)\n","\n","plt.tight_layout()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["f1_scores_per_threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2024-03-02T17:43:50.718688Z","iopub.status.busy":"2024-03-02T17:43:50.717948Z"},"trusted":true},"outputs":[],"source":["вапвап\n","\n","from tqdm import tqdm\n","from collections import defaultdict\n","wandb.init(project=\"RCNN\")\n","\n","id_to_label = {index: label for index, label in enumerate(classes)}\n","\n","f1_scores_per_threshold = defaultdict(lambda: defaultdict(list))\n","recall_scores_per_threshold = defaultdict(lambda: defaultdict(list))\n","\n","confidence_thresholds = [i / 100.0 for i in range(1, 101)]\n","\n","for threshold in tqdm(confidence_thresholds, desc='Evaluating thresholds'):\n","    metric = MeanAveragePrecision(box_format='xyxy', class_metrics=True)\n","    metric.to(device)\n","\n","    confidence_scores_per_class = {class_name: [] for class_name in id_to_label.values()}\n","\n","    model.eval()\n","    with torch.no_grad():\n","        for imgs, targets in test_dl:\n","\n","            imgs = [img.to(device) for img in imgs]\n","            targets = [{k: v.to(device) for (k, v) in d.items()} for d in targets]\n","            predictions = model(imgs)\n","\n","            results = []\n","            for prediction in predictions:\n","                processed_bbox, _ = preprocess_bbox(prediction, threshold)\n","                print(processed_bbox, _)\n","                results.append(processed_bbox)\n","\n","            metric.update(results, targets)\n","\n","    results = metric.compute()\n","    print(\"results: \", results)\n","    # Store F1 scores and recall\n","    for class_idx, class_id in enumerate(results['classes']):\n","        print(\"class_id: \", class_id)\n","\n","        if results['map_per_class'][class_idx] >= 0:  # Check for valid data\n","            precision = results['map_per_class'][class_idx].item()\n","            recall = results['mar_100_per_class'][class_idx].item()\n","            f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","            \n","            class_name = id_to_label[class_id.item()]\n","            f1_scores_per_threshold[class_name][threshold].append(f1_score)\n","\n","            recall_scores_per_threshold[class_name][threshold].append(recall)\n","\n","            wandb.log({\n","                f\"Precision/{class_name}\": precision,\n","                f\"Recall/{class_name}\": recall,\n","                f\"F1/{class_name}\": f1_score,\n","                f\"Average Confidence/{class_name}\": sum(confidence_scores_per_class[class_name]) / len(confidence_scores_per_class[class_name]) if confidence_scores_per_class[class_name] else 0,\n","                \"threshold\": threshold\n","            })\n","\n","# Finish the wandb run\n","wandb.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["for class_name, threshold_f1_scores in f1_scores_per_threshold.items():\n","    print(class_name, threshold_f1_scores)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n","\n","# Plot F1-Confidence\n","for class_name, threshold_f1_scores in f1_scores_per_threshold.items():\n","    thresholds = list(threshold_f1_scores.keys())\n","    f1_scores = [sum(scores) / len(scores) for scores in threshold_f1_scores.values()]  # Calculate mean F1 score per threshold\n","    ax[0].plot(thresholds, f1_scores, label=class_name)\n","\n","ax[0].set_xlabel('Confidence')\n","ax[0].set_ylabel('F1')\n","ax[0].set_title('F1-Confidence')\n","ax[0].legend(loc='upper right', bbox_to_anchor=(1.4, 1.0))  # Adjust legend position\n","ax[0].grid(True)\n","\n","# Plot Recall-Confidence\n","for class_name, threshold_recall_scores in recall_scores_per_threshold.items():\n","    thresholds = list(threshold_recall_scores.keys())\n","    recall_scores = [sum(scores) / len(scores) for scores in threshold_recall_scores.values()]  # Calculate mean recall score per threshold\n","    ax[1].plot(thresholds, recall_scores, label=class_name)\n","\n","ax[1].set_xlabel('Confidence')\n","ax[1].set_ylabel('Recall')\n","ax[1].set_title('Recall-Confidence')\n","ax[1].legend(loc='upper right', bbox_to_anchor=(1.4, 1.0))  # Adjust legend position\n","ax[1].grid(True)\n","\n","plt.tight_layout()\n","plt.show()\n"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":702771,"sourceId":1228192,"sourceType":"datasetVersion"},{"datasetId":1849949,"sourceId":3020523,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"}},"nbformat":4,"nbformat_minor":4}
